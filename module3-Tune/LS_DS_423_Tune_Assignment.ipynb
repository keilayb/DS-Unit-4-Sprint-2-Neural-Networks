{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NGGrt9EYlCqY"
   },
   "source": [
    "\n",
    "\n",
    "# Tune Practice\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 3*\n",
    "\n",
    "# Gridsearch Hyperparameters\n",
    "\n",
    "In the guided project, you learned how to use sklearn's GridsearchCV and keras-tuner library to tune the hyperparamters of a neural network model. For your module project you'll continue using these two libraries however we are going to make things a little more interesting for you. \n",
    "\n",
    "Continue to use TensorFlow Keras & a sample of the [Quickdraw dataset](https://github.com/googlecreativelab/quickdraw-dataset) to build a sketch classification model. The dataset has been sampled to only 10 classes and 10000 observations per class. \n",
    "\n",
    "\n",
    "\n",
    "**Don't forgot to switch to GPU on Colab!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# native python libraries imports \n",
    "import math\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# sklearn imports \n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# keras imports \n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from kerastuner.tuners import RandomSearch, BayesianOptimization, Sklearn\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "from keras.activations import relu, sigmoid\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.utils import get_file\n",
    "\n",
    "# required for compatibility between sklearn and keras\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_quickdraw10():\n",
    "    \"\"\"\n",
    "    Loads a sample of the Quickdraw dataset, which can be found at:\n",
    "    https://github.com/LambdaSchool/DS-Unit-4-Sprint-2-Neural-Networks/blob/main/quickdraw10.npz?raw=true\n",
    "    \n",
    "    After loading the data, the data is normalized, and split into the feature matrix\n",
    "    and target vector.\n",
    "    \n",
    "    The feature matrix and target vector then undergo shuffled splitting into a train\n",
    "    and test data set.\n",
    "    \n",
    "    Returned:\n",
    "    ---------\n",
    "    normalized feature matrix train and test\n",
    "    target vector train and test\n",
    "    \"\"\"\n",
    "    \n",
    "    # url where data can be found\n",
    "    URL_ = \"https://github.com/LambdaSchool/DS-Unit-4-Sprint-2-Neural-Networks/blob/main/quickdraw10.npz?raw=true\"\n",
    "    \n",
    "    # create path to the file\n",
    "    path_to_zip = get_file('./quickdraw10.npz', origin=URL_, extract=False)\n",
    "\n",
    "    # load in data\n",
    "    data = np.load(path_to_zip)\n",
    "    \n",
    "    # normalize your image data\n",
    "    max_pixel_value = 255\n",
    "    X = data['arr_0']/max_pixel_value # create normalized feature matrix\n",
    "    Y = data['arr_1'] # create target vector\n",
    "        \n",
    "    # split feature matrix and target vector in train and test data sets\n",
    "    # shuffle the data before the split\n",
    "    return train_test_split(X, Y, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_quickdraw10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75000, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "\n",
    "# Experiment 1\n",
    "\n",
    "## Tune Hyperperameters using Enhanced GridsearchCV \n",
    "\n",
    "We are going to use GridsearchCV again to tune a deep learning model however we are going to add some additional functionality to our gridsearch. Specifically, we are going to automate away the generation of how many nodes to use in a layer and how many layers to use in a model! \n",
    "\n",
    "By the way, yes, there is a function within a function. Try to not let that bother you. An alternative to this would be to create a class. If you're up for the challenge give it a shot. However, consider this a stretch goal that you come back to after you finish going through this assignment. \n",
    "\n",
    "\n",
    "### Objective \n",
    "\n",
    "The objective of this experiment is to show you how to automate the generation of layers and layer nodes for the purposes of gridsearch. Up until now, we've been manually selecting the number of layers and layer nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "USXjs7Hk71Hy"
   },
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(n_layers,  first_layer_nodes, last_layer_nodes, act_funct =\"relu\", negative_node_incrementation=True):\n",
    "    \"\"\"\"\n",
    "    Returns a complied keras model \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_layers: int \n",
    "        number of hidden layers in model \n",
    "        To be clear, this excludes the input and output layer.\n",
    "        \n",
    "    first_layer_nodes: int\n",
    "        Number of nodes in the first hidden layer \n",
    "\n",
    "    last_layer_nodes: int\n",
    "        Number of nodes in the last hidden layer (this is the layer just prior to the output layer)\n",
    "        \n",
    "     act_funct: string \n",
    "         Name of activation function to use in hidden layers (this excludes the output layler)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    model: keras object \n",
    "    \"\"\"\n",
    "    \n",
    "    def gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation=True):\n",
    "        \"\"\"\n",
    "        Generates and returns the number of nodes in each hidden layer. \n",
    "        To be clear, this excludes the input and output layer. \n",
    "\n",
    "        Note\n",
    "        ----\n",
    "        Number of nodes in each layer is linearly incremented. \n",
    "        For example, gen_layer_nodes(5, 500, 100) will generate [500, 400, 300, 200, 100]\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_layers: int\n",
    "            Number of hidden layers\n",
    "            This values should be 2 or greater \n",
    "\n",
    "        first_layer_nodes: int\n",
    "\n",
    "        last_layer_nodes: int\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        layers: list of ints\n",
    "            Contains number of nodes for each layer \n",
    "        \"\"\"\n",
    "\n",
    "        # throws an error if n_layers is less than 2 \n",
    "        assert n_layers >= 2, \"n_layers must be 2 or greater\"\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # PROTIP: IF YOU WANT THE NODE INCREMENTATION TO BE SPACED DIFFERENTLY\n",
    "        # THEN YOU'LL NEED TO CHANGE THE WAY THAT IT'S CALCULATED - HAVE FUN!\n",
    "        # when set to True number of nodes are decreased for subsequent layers \n",
    "        if negative_node_incrementation:\n",
    "            # subtract this amount from previous layer's nodes in order to increment towards smaller numbers \n",
    "            nodes_increment = (last_layer_nodes - first_layer_nodes)/ (n_layers-1)\n",
    "            \n",
    "        # when set to False number of nodes are increased for subsequent layers\n",
    "        else:\n",
    "            # add this amount from previous layer's nodes in order to increment towards larger numbers \n",
    "            nodes_increment = (first_layer_nodes - last_layer_nodes)/ (n_layers-1)\n",
    "\n",
    "        nodes = first_layer_nodes\n",
    "\n",
    "        for i in range(1, n_layers+1):\n",
    "\n",
    "            layers.append(math.ceil(nodes))\n",
    "\n",
    "            # increment nodes for next layer \n",
    "            nodes = nodes + nodes_increment\n",
    "\n",
    "        return layers\n",
    "    \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    \n",
    "    n_nodes = gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation)\n",
    "    \n",
    "    for i in range(1, n_layers):\n",
    "        if i==1:\n",
    "            model.add(Dense(first_layer_nodes, input_dim=X_train.shape[1], activation=act_funct))\n",
    "        else:\n",
    "            model.add(Dense(n_nodes[i-1], activation=act_funct))\n",
    "            \n",
    "            \n",
    "    # output layer \n",
    "    model.add(Dense(10, # 10 unit/neurons in output layer because we have 10 possible labels to predict  \n",
    "                    activation='softmax')) # use softmax for a label set greater than 2            \n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', \n",
    "                  optimizer='adam', # adam is a good default optimizer \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # do not include model.fit() inside the create_model function\n",
    "    # KerasClassifier is expecting a complied model \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore create_model\n",
    "\n",
    "Let's build a few different models in order to understand how the above code works in practice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model \n",
    "\n",
    "Use `create_model` to build a model. \n",
    "\n",
    "- Set `n_layers = 10` \n",
    "- Set `first_layer_nodes = 500`\n",
    "- Set `last_layer_nodes = 100`\n",
    "- Set `act_funct = \"relu\"`\n",
    "- Make sure that `negative_node_incrementation = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5dcf5c585f07629a03086cf57ba53615",
     "grade": false,
     "grade_id": "cell-86d63e89a21223de",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# use create_model to create a model \n",
    "model = create_model(n_layers=10, \n",
    "                     first_layer_nodes=500,\n",
    "                     last_layer_nodes=100,\n",
    "                     act_funct =\"relu\", \n",
    "                     negative_node_incrementation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 456)               228456    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 412)               188284    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 367)               151571    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 323)               118864    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 278)               90072     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 234)               65286     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 189)               44415     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 145)               27550     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                1460      \n",
      "=================================================================\n",
      "Total params: 1,308,458\n",
      "Trainable params: 1,308,458\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# run model.summary() and make sure that you understand the model architecture that you just built \n",
    "# Notice in the model summary how the number of nodes have been linearly incremented in decreasing values. \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model \n",
    "\n",
    "Use `create_model` to build a model. \n",
    "\n",
    "- Set `n_layers = 10` \n",
    "- Set `first_layer_nodes = 500`\n",
    "- Set `last_layer_nodes = 100`\n",
    "- Set `act_funct = \"relu\"`\n",
    "- Make sure that `negative_node_incrementation = False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0722533c325d699f4842e874e43720e",
     "grade": false,
     "grade_id": "cell-99d563a291231a7b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# use create_model to create a model \n",
    "model = create_model(n_layers=10, \n",
    "                     first_layer_nodes=500,\n",
    "                     last_layer_nodes=100,\n",
    "                     act_funct =\"relu\", \n",
    "                     negative_node_incrementation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 545)               273045    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 589)               321594    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 634)               374060    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 678)               430530    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 723)               490917    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 767)               555308    \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 812)               623616    \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 856)               695928    \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 10)                8570      \n",
      "=================================================================\n",
      "Total params: 4,166,068\n",
      "Trainable params: 4,166,068\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# run model.summary() and make sure that you understand the model architecture that you just built \n",
    "# Notice in the model summary how the number of nodes have been linearly incremented in increasing values.\n",
    "# The output layer must have 10 nodes because there are 10 labels to predict \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we've played around a bit with  `create_model` in order to understand how it works, let's build a much simpler model that we'll be running gridsearches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model \n",
    "\n",
    "Use `create_model` to build a model. \n",
    "\n",
    "- Set `n_layers = 2` \n",
    "- Set `first_layer_nodes = 500`\n",
    "- Set `last_layer_nodes = 100`\n",
    "- Set `act_funct = \"relu\"`\n",
    "- Make sure that `negative_node_incrementation = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "606b85d0ba4531836f97caf6850297f8",
     "grade": false,
     "grade_id": "cell-4ca6c5e51302fd10",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# use create_model to create a model \n",
    "model = create_model(n_layers=2, \n",
    "                     first_layer_nodes=500,\n",
    "                     last_layer_nodes=100,\n",
    "                     act_funct =\"relu\", \n",
    "                     negative_node_incrementation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_22 (Dense)             (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 10)                5010      \n",
      "=================================================================\n",
      "Total params: 397,510\n",
      "Trainable params: 397,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# run model.summary() and make sure that you understand the model architecture that you just built \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid search parameters\n",
    "param_grid = {'n_layers': [2, 3],\n",
    "              'epochs': [3], \n",
    "              \"first_layer_nodes\": [500, 300],\n",
    "              \"last_layer_nodes\": [100, 50]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(create_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Epoch 1/3\n",
      "2344/2344 [==============================] - 5s 2ms/step - loss: 0.7709 - accuracy: 0.7648\n",
      "Epoch 2/3\n",
      "2344/2344 [==============================] - 4s 2ms/step - loss: 0.4228 - accuracy: 0.8741\n",
      "Epoch 3/3\n",
      "2344/2344 [==============================] - 4s 2ms/step - loss: 0.3389 - accuracy: 0.8991\n",
      "Best: 0.8660533428192139 using {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 2}\n",
      "Means: 0.8648933172225952, Stdev: 0.003692923752483053 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 2}\n",
      "Means: 0.865933338801066, Stdev: 0.0009590753884723514 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 3}\n",
      "Means: 0.8660533428192139, Stdev: 0.0024547819066268834 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 2}\n",
      "Means: 0.8656399846076965, Stdev: 0.001301482958733984 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 3}\n",
      "Means: 0.8616000016530355, Stdev: 0.0010203041373970016 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 100, 'n_layers': 2}\n",
      "Means: 0.8633333245913187, Stdev: 0.0025667285290341692 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 100, 'n_layers': 3}\n",
      "Means: 0.8612666527430216, Stdev: 0.005145072797145499 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 50, 'n_layers': 2}\n",
      "Means: 0.8616933425267538, Stdev: 0.001180433568166816 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 50, 'n_layers': 3}\n"
     ]
    }
   ],
   "source": [
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, \n",
    "                    param_grid=param_grid, \n",
    "                    n_jobs=-1, \n",
    "                    verbose=1, \n",
    "                    cv=3)\n",
    "\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_result.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epochs': 3,\n",
       " 'first_layer_nodes': 500,\n",
       " 'last_layer_nodes': 50,\n",
       " 'n_layers': 2,\n",
       " 'build_fn': <function __main__.create_model(n_layers, first_layer_nodes, last_layer_nodes, act_funct='relu', negative_node_incrementation=True)>}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Experiment 2\n",
    "\n",
    "## Benchmark different Optimization Algorithms \n",
    "\n",
    "In this section, we are going to use the same model and dataset in order to benchmark 3 different gridsearch approaches: \n",
    "\n",
    "- Random Search\n",
    "- Bayesian Optimization. \n",
    "- Brute Force Gridsearch\n",
    "\n",
    "Our goal in this experiment is two-fold. We want to see which appraoch \n",
    "\n",
    "- Scores the highest accuracy\n",
    "- Has the shortest run time \n",
    "\n",
    "We want to see how these 3 gridsearch approaches handle these trade-offs and to give you a sense of those trades offs.\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "`Brute Force Gridsearch` will train a model on every single unique hyperparameter combination, this guarantees that you'll get the highest possible accuracy from your parameter set but your gridsearch might have a very long run-time. \n",
    "\n",
    "`Random Search` will randomly sample from your parameter set which, depending on how many samples, the run-time might be significantly cut down but you might or might not sample the parameters that correspond to the heightest possible accuracies. \n",
    "\n",
    "`Bayesian Optimization` has a bit of intelligence built into it's search algorithm but you do need to manually select some parameters which greatly influence the model learning outcomes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "### Build our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because gridsearching can take a lot of time and we are bench marking 3 different approaches\n",
    "# let's build a simple model to minimize run time \n",
    "\n",
    "def build_model(hp):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a complied keras model ready for keras-tuner gridsearch algorithms \n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # hidden layer\n",
    "    model.add(Dense(units=hp.get('units'),activation=hp.get(\"activation\")))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(hp.get('learning_rate')),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'relu'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build out our hyperparameter dictionary \n",
    "hp = HyperParameters()\n",
    "hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "hp.Choice('learning_rate',values=[1e-1, 1e-2, 1e-3])\n",
    "hp.Choice('activation',values=[\"relu\", \"sigmoid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# Run the Gridsearch Algorithms \n",
    "\n",
    "### Random Search\n",
    "\n",
    "Be sure to check out the [**docs for Keras-Tuner**](https://keras-team.github.io/keras-tuner/documentation/tuners/). Here you can read about the input parameters for the `RandomSearch` tuner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aaff9aae33845f374e15f2381719d83a",
     "grade": false,
     "grade_id": "cell-8c1dfb9b6d12bea2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# how many unique hyperparameter combinations do we have? \n",
    "# HINT: take the product of the number of possible values for each hyperparameter \n",
    "# save your answer to n_unique_hparam_combos\n",
    "n_unique_hparam_combos = 16 * 3 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9d628451e83431e1b52da10eccf2c00",
     "grade": false,
     "grade_id": "cell-1fa83950bb2d5f92",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# how many of these do we want to randomly sample?\n",
    "# let's pick 25% of n_unique_hparam_combos param combos to sample\n",
    "# save this number to n_param_combos_to_sample\n",
    "n_param_combos_to_sample = n_unique_hparam_combos*0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tuner = RandomSearch(\n",
    "            build_model,\n",
    "            objective='val_accuracy',\n",
    "            max_trials=n_param_combos_to_sample, # number of times to sample the parameter set and build a model \n",
    "            seed=1234,\n",
    "            hyperparameters=hp, # pass in our hyperparameter dictionary\n",
    "            directory='./keras-tuner-trial',\n",
    "            project_name='random_search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 24 Complete [00h 00m 08s]\n",
      "val_accuracy: 0.6904799938201904\n",
      "\n",
      "Best val_accuracy So Far: 0.8688399791717529\n",
      "Total elapsed time: 00h 03m 53s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "# take note of Total elapsed time in print out\n",
    "random_tuner.search(X_train, y_train,\n",
    "                    epochs=3,\n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in ./keras-tuner-trial/random_search\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 352\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "Score: 0.8688399791717529\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 384\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "Score: 0.863319993019104\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 448\n",
      "learning_rate: 0.001\n",
      "activation: sigmoid\n",
      "Score: 0.860759973526001\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 480\n",
      "learning_rate: 0.001\n",
      "activation: sigmoid\n",
      "Score: 0.8600800037384033\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 416\n",
      "learning_rate: 0.001\n",
      "activation: sigmoid\n",
      "Score: 0.8595200181007385\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 288\n",
      "learning_rate: 0.001\n",
      "activation: sigmoid\n",
      "Score: 0.8557199835777283\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 224\n",
      "learning_rate: 0.001\n",
      "activation: sigmoid\n",
      "Score: 0.85343998670578\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 160\n",
      "learning_rate: 0.01\n",
      "activation: sigmoid\n",
      "Score: 0.8360000252723694\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 320\n",
      "learning_rate: 0.01\n",
      "activation: sigmoid\n",
      "Score: 0.8357999920845032\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 192\n",
      "learning_rate: 0.01\n",
      "activation: sigmoid\n",
      "Score: 0.8355200290679932\n"
     ]
    }
   ],
   "source": [
    "# identify the best score and hyperparamter (should be at the top since scores are ranked)\n",
    "random_tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    " ### Results\n",
    " \n",
    "Identify and write the the best performing hyperparamter combination and model score. \n",
    "Note that because this is Random Search, multiple runs might have slighly different outcomes. \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f084b5d373f8589a1de8d6d4473b974a",
     "grade": true,
     "grade_id": "cell-5527738b6382c164",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Best Performing Hyperparameter Combination--RandomSearch:**\n",
    "\n",
    "units: 416\n",
    "\n",
    "learning_rate: 0.001\n",
    "\n",
    "activation: sigmoid\n",
    "\n",
    "\n",
    "**Best Score:** 0.8595200181007385"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Bayesian Optimization\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/0/02/GpParBayesAnimationSmall.gif)\n",
    "\n",
    "Be sure to check out the [**docs for Keras-Tuner**](https://keras-team.github.io/keras-tuner/documentation/tuners/). Here you can read about the input parameters for the `BayesianOptimization` tuner.\n",
    "\n",
    "Pay special attention to these `BayesianOptimization` parameters: `num_initial_points` and `beta`. \n",
    "\n",
    "`num_initial_points`: \n",
    "\n",
    "Number of randomly selected hyperparameter combinations to try before applying bayesian probability to determine liklihood of which param combo to try next based on expected improvement\n",
    "\n",
    "\n",
    "`beta`: \n",
    "\n",
    "Larger values means more willing to explore new hyperparameter combinations (analogous to searching for the global minimum in Gradient Descent), smaller values means that it is less willing to try new hyperparameter combinations (analogous to getting stuck in a local minimum in Gradient Descent). \n",
    "\n",
    "As a start, error on the side of larger values. What defines a small or large value you ask? That question would pull us into the mathematical intricacies of Bayesian Optimization and Gaussian Processes. For simplicity, notice that the default value is 2.6 and work from there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we know that 24 samples is about 25% of 96 possible hyper-parameter combos\n",
    "# because BO isn't random (after num_initial_points number of trails) let's see if 15 max trials gives good results\n",
    "# feel free to play with any of these numbers\n",
    "max_trials=15\n",
    "num_initial_points=5\n",
    "beta=5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_tuner = BayesianOptimization(\n",
    "                    build_model,\n",
    "                    objective='val_accuracy',\n",
    "                    max_trials=max_trials,\n",
    "                    hyperparameters=hp, # pass in our hyperparameter dictionary\n",
    "                    num_initial_points=num_initial_points, \n",
    "                    beta=beta, \n",
    "                    seed=1234,\n",
    "                    directory='./keras-tuner-trial',\n",
    "                    project_name='bayesian_optimization_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15 Complete [00h 00m 11s]\n",
      "val_accuracy: 0.8248400092124939\n",
      "\n",
      "Best val_accuracy So Far: 0.8712800145149231\n",
      "Total elapsed time: 00h 02m 31s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "bayesian_tuner.search(X_train, y_train,\n",
    "               epochs=3,\n",
    "               validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in ./keras-tuner-trial/bayesian_optimization_4\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 512\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "Score: 0.8712800145149231\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 352\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "Score: 0.8693199753761292\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 192\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "Score: 0.8649200201034546\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 480\n",
      "learning_rate: 0.001\n",
      "activation: sigmoid\n",
      "Score: 0.8603600263595581\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 256\n",
      "learning_rate: 0.001\n",
      "activation: sigmoid\n",
      "Score: 0.8510800004005432\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 256\n",
      "learning_rate: 0.01\n",
      "activation: sigmoid\n",
      "Score: 0.8364400267601013\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 512\n",
      "learning_rate: 0.01\n",
      "activation: relu\n",
      "Score: 0.8248400092124939\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 32\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "Score: 0.8230000138282776\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 384\n",
      "learning_rate: 0.01\n",
      "activation: relu\n",
      "Score: 0.8194000124931335\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 32\n",
      "learning_rate: 0.001\n",
      "activation: sigmoid\n",
      "Score: 0.8058000206947327\n"
     ]
    }
   ],
   "source": [
    "bayesian_tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Results\n",
    " \n",
    "Identify and write the the best performing hyperparamter combination and model score. \n",
    "Note that because this is  Bayesian Optimization, multiple runs might have slighly different outcomes. \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1badcdca408cdd49bc2e409dca3bac5a",
     "grade": true,
     "grade_id": "cell-ff95600bf745f40f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Best Performing Hyperparameter Combination--BayesianOptimization:**\n",
    "\n",
    "units: 512\n",
    "\n",
    "learning_rate: 0.001\n",
    "\n",
    "activation: relu\n",
    "\n",
    "**Best Score:** 0.8712800145149231"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "## Brute Force Gridsearch Optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate a Sklearn compatiable parameter dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build out our hyperparameter dictionary \n",
    "hyper_parameters = {\n",
    "    # BUG Fix: cast array as list otherwise GridSearchCV will throw error\n",
    "    \"units\": np.arange(32, 544, 32).tolist(),\n",
    "    \"learning_rate\": [1e-1, 1e-2, 1e-3],\n",
    "    \"activation\":[\"relu\", \"sigmoid\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'units': [32,\n",
       "  64,\n",
       "  96,\n",
       "  128,\n",
       "  160,\n",
       "  192,\n",
       "  224,\n",
       "  256,\n",
       "  288,\n",
       "  320,\n",
       "  352,\n",
       "  384,\n",
       "  416,\n",
       "  448,\n",
       "  480,\n",
       "  512],\n",
       " 'learning_rate': [0.1, 0.01, 0.001],\n",
       " 'activation': ['relu', 'sigmoid']}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Sklearn compatiable model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(units, learning_rate, activation):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a complie keras model ready for keras-tuner gridsearch algorithms \n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # hidden layer\n",
    "    model.add(Dense(units, activation=activation))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn = build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 96 candidates, totalling 288 fits\n",
      "2344/2344 [==============================] - 3s 1ms/step - loss: 0.7695 - accuracy: 0.7652\n",
      "Best: 0.8449599941571554 using {'activation': 'relu', 'learning_rate': 0.001, 'units': 480}\n",
      "Means: 0.2900266647338867, Stdev: 0.01737683738467732 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 32}\n",
      "Means: 0.25197334090868634, Stdev: 0.05959962968823266 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 64}\n",
      "Means: 0.20678666730721793, Stdev: 0.03748037823677755 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 96}\n",
      "Means: 0.2860266665617625, Stdev: 0.04897847250512216 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 128}\n",
      "Means: 0.2573999911546707, Stdev: 0.03133106985928215 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 160}\n",
      "Means: 0.23365333179632822, Stdev: 0.05492940521764439 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 192}\n",
      "Means: 0.24677333235740662, Stdev: 0.06660981615580537 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 224}\n",
      "Means: 0.2984000047047933, Stdev: 0.026423281875898548 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 256}\n",
      "Means: 0.3018133342266083, Stdev: 0.023943423160539594 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 288}\n",
      "Means: 0.2757333368062973, Stdev: 0.0647192935058351 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 320}\n",
      "Means: 0.3051066646973292, Stdev: 0.0476487862742391 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 352}\n",
      "Means: 0.27700000007947284, Stdev: 0.04120206358801408 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 384}\n",
      "Means: 0.28517333666483563, Stdev: 0.03301138979191432 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 416}\n",
      "Means: 0.26843999822934467, Stdev: 0.009212695057146936 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 448}\n",
      "Means: 0.287840003768603, Stdev: 0.04385319235983998 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 480}\n",
      "Means: 0.29736000299453735, Stdev: 0.026160269149403385 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 512}\n",
      "Means: 0.7850800156593323, Stdev: 0.005944362171481705 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 32}\n",
      "Means: 0.7993866602579752, Stdev: 0.005611626323357228 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 64}\n",
      "Means: 0.8019866744677225, Stdev: 0.01280191991317467 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 96}\n",
      "Means: 0.801146666208903, Stdev: 0.007466565013469801 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 128}\n",
      "Means: 0.7973066568374634, Stdev: 0.013104742084888538 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 160}\n",
      "Means: 0.8033466736475626, Stdev: 0.005538760656783434 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 192}\n",
      "Means: 0.8044933279355367, Stdev: 0.001811774459512009 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 224}\n",
      "Means: 0.806013305981954, Stdev: 0.012199466697473892 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 256}\n",
      "Means: 0.8038933277130127, Stdev: 0.0060625234171741834 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 288}\n",
      "Means: 0.8062933484713236, Stdev: 0.0019504207136828145 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 320}\n",
      "Means: 0.8065333366394043, Stdev: 0.002338397865574916 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 352}\n",
      "Means: 0.7985199888547262, Stdev: 0.0027772520224146943 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 384}\n",
      "Means: 0.8047866622606913, Stdev: 0.00576058143626201 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 416}\n",
      "Means: 0.8109466632207235, Stdev: 0.002496159111510236 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 448}\n",
      "Means: 0.8010533452033997, Stdev: 0.005123165644056041 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 480}\n",
      "Means: 0.7916799982388815, Stdev: 0.01245740039933834 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 512}\n",
      "Means: 0.7849599917729696, Stdev: 0.004267214939030157 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 32}\n",
      "Means: 0.8088800112406412, Stdev: 0.002731170852375676 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 64}\n",
      "Means: 0.823586662610372, Stdev: 0.0014014422998291781 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 96}\n",
      "Means: 0.824239989121755, Stdev: 0.0011318330891344002 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 128}\n",
      "Means: 0.8303200006484985, Stdev: 0.0013010732135312988 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 160}\n",
      "Means: 0.8313733339309692, Stdev: 0.004553257171275033 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 192}\n",
      "Means: 0.8350800077120463, Stdev: 0.003369981507855099 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 224}\n",
      "Means: 0.8380533258120219, Stdev: 0.001646118408976185 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 256}\n",
      "Means: 0.8354533513387045, Stdev: 0.0020083409357864474 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 288}\n",
      "Means: 0.837879995505015, Stdev: 0.003979144529669904 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 320}\n",
      "Means: 0.8401333292325338, Stdev: 0.002551918332798161 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 352}\n",
      "Means: 0.8420266707738241, Stdev: 0.0012591370857038613 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 384}\n",
      "Means: 0.838480015595754, Stdev: 0.00390663444945244 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 416}\n",
      "Means: 0.8416666587193807, Stdev: 0.006727733121919933 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 448}\n",
      "Means: 0.8449599941571554, Stdev: 0.003351586243449769 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 480}\n",
      "Means: 0.8375733494758606, Stdev: 0.01065965049975603 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 512}\n",
      "Means: 0.6612799962361654, Stdev: 0.011875671119141977 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 32}\n",
      "Means: 0.6470400094985962, Stdev: 0.02998275346053115 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 64}\n",
      "Means: 0.6741199890772501, Stdev: 0.01929264674814464 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 96}\n",
      "Means: 0.664786676565806, Stdev: 0.04188945426299493 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 128}\n",
      "Means: 0.6293466687202454, Stdev: 0.03394723784173288 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 160}\n",
      "Means: 0.6704933245976766, Stdev: 0.007174937786381936 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 192}\n",
      "Means: 0.6550133228302002, Stdev: 0.011762819039330557 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 224}\n",
      "Means: 0.642853319644928, Stdev: 0.025713541217572678 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 256}\n",
      "Means: 0.6771333416302999, Stdev: 0.01993097773077607 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 288}\n",
      "Means: 0.654199997584025, Stdev: 0.023343718229375407 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 320}\n",
      "Means: 0.6563066840171814, Stdev: 0.024696435252434246 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 352}\n",
      "Means: 0.5863199830055237, Stdev: 0.01848059986887349 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 384}\n",
      "Means: 0.6113866766293844, Stdev: 0.02778920363326862 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 416}\n",
      "Means: 0.6112266580263773, Stdev: 0.013243636883709995 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 448}\n",
      "Means: 0.6331733266512553, Stdev: 0.04673938549337603 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 480}\n",
      "Means: 0.6585466663042704, Stdev: 0.022720020207781407 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 512}\n",
      "Means: 0.7934133410453796, Stdev: 0.000630193532294621 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 32}\n",
      "Means: 0.8066666722297668, Stdev: 0.002056365604554899 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 64}\n",
      "Means: 0.8161333401997884, Stdev: 0.0012784866518575464 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 96}\n",
      "Means: 0.8130666812260946, Stdev: 0.0012433461145475663 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 128}\n",
      "Means: 0.8152666687965393, Stdev: 0.0005429164053173533 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 160}\n",
      "Means: 0.8157066702842712, Stdev: 0.0026972339151249657 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 192}\n",
      "Means: 0.8095999956130981, Stdev: 0.01177971048262156 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 224}\n",
      "Means: 0.8176400065422058, Stdev: 0.0018593305984940507 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 256}\n",
      "Means: 0.8074399828910828, Stdev: 0.004175749162547333 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 288}\n",
      "Means: 0.8120533227920532, Stdev: 0.008288937995794688 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 320}\n",
      "Means: 0.8111333250999451, Stdev: 0.00919373434333103 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 352}\n",
      "Means: 0.8186799883842468, Stdev: 0.002449049746339167 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 384}\n",
      "Means: 0.8227466742197672, Stdev: 0.002314540622683914 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 416}\n",
      "Means: 0.8153866728146871, Stdev: 0.0016603001390751437 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 448}\n",
      "Means: 0.8135066628456116, Stdev: 0.004360340879080639 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 480}\n",
      "Means: 0.8157733281453451, Stdev: 0.0032040490529285856 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 512}\n",
      "Means: 0.7626533309618632, Stdev: 0.0006711801067942154 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 32}\n",
      "Means: 0.7790799935658773, Stdev: 0.0035067823339254853 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 64}\n",
      "Means: 0.7851466536521912, Stdev: 0.0012116358133504694 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 96}\n",
      "Means: 0.7899333238601685, Stdev: 0.0038258400640989705 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 128}\n",
      "Means: 0.797160009543101, Stdev: 0.0016359436105087505 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 160}\n",
      "Means: 0.7949999769528707, Stdev: 0.0031373869568015306 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 192}\n",
      "Means: 0.7948533495267233, Stdev: 0.0025993486287805155 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 224}\n",
      "Means: 0.7983200152715048, Stdev: 0.004587641981491207 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 256}\n",
      "Means: 0.7992133299509684, Stdev: 0.002006470825868174 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 288}\n",
      "Means: 0.8024933139483134, Stdev: 0.0029077083733735666 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 320}\n",
      "Means: 0.8025733431180319, Stdev: 0.0004505756245128801 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 352}\n",
      "Means: 0.8020933270454407, Stdev: 0.005254522607827364 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 384}\n",
      "Means: 0.7926133473714193, Stdev: 0.007358714131098156 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 416}\n",
      "Means: 0.8051200111707052, Stdev: 0.0014631381669958272 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 448}\n",
      "Means: 0.8025333285331726, Stdev: 0.0039068255098022335 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 480}\n",
      "Means: 0.8007599910100301, Stdev: 0.0014543767901237276 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 512}\n"
     ]
    }
   ],
   "source": [
    "# save start time \n",
    "start = time()\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, \n",
    "                    param_grid=hyper_parameters, \n",
    "                    n_jobs=-2, \n",
    "                    verbose=1, \n",
    "                    cv=3)\n",
    "\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# save end time \n",
    "end = time()\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.250651001930237"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total run time \n",
    "total_run_time_in_miniutes = (end - start)/60\n",
    "total_run_time_in_miniutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu', 'learning_rate': 0.001, 'units': 480}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 1s 769us/step - loss: 0.5074 - accuracy: 0.8509\n"
     ]
    }
   ],
   "source": [
    "# because all other optimization approaches are reporting test set score\n",
    "# let's calculate the test set score in this case \n",
    "best_model = grid_result.best_estimator_\n",
    "test_acc = best_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8508800268173218"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Results\n",
    " \n",
    "Identify and write the the best performing hyperparamter combination and model score. \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9577db883482c6cded3836e5cfbf5a74",
     "grade": true,
     "grade_id": "cell-eb06d682d2790f6e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Best Performing Hyperparameter Combination--GridSearchCV:**\n",
    "\n",
    "units: 480\n",
    "\n",
    "learning_rate: 0.001\n",
    "\n",
    "activation: relu\n",
    "\n",
    "**Best Score:** 0.8508800268173218"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "The spirit of this experiment is to expose you to the idea of benchmarking and comparing the trade-offs of various gridsearch approaches. \n",
    "\n",
    "Even if we did find a way to pass in the original test set into GridSearchCV, we can see that both Random Search and Bayesian Optimization are arguably better alternatives to a brute force grid search when we consider the trade-offs of run time and locating the best performing model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "\n",
    "# Stretch Goals\n",
    "\n",
    "- Feel free to run whatever gridserach experiments on whatever models you like!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark differenct Optimization Algorithms and different Model Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create my own function to create a model\n",
    "def create_model_k(hp_k):\n",
    "    \"\"\"\"\n",
    "    Returns a complied keras model \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hp_k: hyperparameter dictionary of HyperParameter class\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    model: keras object \n",
    "    \"\"\"\n",
    "    \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(hp_k.get('first_layer_nodes'), input_dim=X_train.shape[1], activation='relu'))\n",
    "            \n",
    "    # output layer \n",
    "    model.add(Dense(10, # 10 unit/neurons in output layer because we have 10 possible labels to predict  \n",
    "                    activation='softmax')) # use softmax for a label set greater than 2            \n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', \n",
    "                  optimizer=keras.optimizers.Adam(hp.get('learning_rate')), # adam is a good default optimizer \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # do not include model.fit() inside the create_model function\n",
    "    # KerasClassifier is expecting a complied model \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build my own hyperparameter dictionary \n",
    "hp_k = HyperParameters()\n",
    "hp_k.Choice('first_layer_nodes', values=[275, 300, 450, 512, 550, 600])\n",
    "hp_k.Choice('learning_rate',values=[0.1, 0.05, 0.001])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Gridsearch Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aaff9aae33845f374e15f2381719d83a",
     "grade": false,
     "grade_id": "cell-8c1dfb9b6d12bea2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# how many unique hyperparameter combinations do we have? \n",
    "# HINT: take the product of the number of possible values for each hyperparameter \n",
    "# save your answer to n_unique_hparam_combos\n",
    "n_unique_hparam_combos = 6 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9d628451e83431e1b52da10eccf2c00",
     "grade": false,
     "grade_id": "cell-1fa83950bb2d5f92",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# how many of these do we want to randomly sample?\n",
    "# let's pick 25% of n_unique_hparam_combos param combos to sample\n",
    "# save this number to n_param_combos_to_sample\n",
    "n_param_combos_to_sample = n_unique_hparam_combos*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tuner_k5 = RandomSearch(\n",
    "            create_model_k,\n",
    "            objective='val_accuracy',\n",
    "            max_trials=n_param_combos_to_sample, # number of times to sample the parameter set and build a model \n",
    "            seed=567,\n",
    "            hyperparameters=hp_k, # pass in our hyperparameter dictionary\n",
    "            directory='./keras-tuner-trial-stretch-goal',\n",
    "            project_name='random_search_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 18 Complete [00h 00m 18s]\n",
      "val_accuracy: 0.2539199888706207\n",
      "\n",
      "Best val_accuracy So Far: 0.32708001136779785\n",
      "Total elapsed time: 00h 05m 35s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "# take note of Total elapsed time in print out\n",
    "random_tuner_k5.search(X_train, y_train,\n",
    "                     epochs=5,\n",
    "                     validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in ./keras-tuner-trial-stretch-goal/random_search_5\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "first_layer_nodes: 300\n",
      "learning_rate: 0.1\n",
      "Score: 0.32708001136779785\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "first_layer_nodes: 512\n",
      "learning_rate: 0.05\n",
      "Score: 0.3051599860191345\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "first_layer_nodes: 275\n",
      "learning_rate: 0.1\n",
      "Score: 0.3022400140762329\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "first_layer_nodes: 600\n",
      "learning_rate: 0.001\n",
      "Score: 0.3012399971485138\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "first_layer_nodes: 450\n",
      "learning_rate: 0.075\n",
      "Score: 0.2990800142288208\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "first_layer_nodes: 550\n",
      "learning_rate: 0.1\n",
      "Score: 0.29820001125335693\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "first_layer_nodes: 300\n",
      "learning_rate: 0.001\n",
      "Score: 0.2938399910926819\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "first_layer_nodes: 275\n",
      "learning_rate: 0.001\n",
      "Score: 0.2937999963760376\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "first_layer_nodes: 275\n",
      "learning_rate: 0.075\n",
      "Score: 0.2752000093460083\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "first_layer_nodes: 600\n",
      "learning_rate: 0.075\n",
      "Score: 0.2674799859523773\n"
     ]
    }
   ],
   "source": [
    "# identify the best score and hyperparamter (should be at the top since scores are ranked)\n",
    "random_tuner_k5.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "**Attempt 1**\n",
    "\n",
    "**Best Hyperparameters--RandomSearch:**\n",
    "\n",
    "n_layers: 4\n",
    "\n",
    "first_layer_nodes: 300\n",
    "\n",
    "last_layer_nodes: 50\n",
    "\n",
    "learning_rate: 0.01\n",
    "\n",
    "activation: relu\n",
    "\n",
    "**Best Score:** 0.24476000666618347"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attempt 2**\n",
    "\n",
    "- Increased number of epochs and trials\n",
    "\n",
    "**Best Hyperparameters--RandomSearch:**\n",
    "\n",
    "n_layers: 3\n",
    "\n",
    "first_layer_nodes: 300\n",
    "\n",
    "last_layer_nodes: 100\n",
    "\n",
    "learning_rate: 0.1\n",
    "\n",
    "activation: relu\n",
    "\n",
    "**Best Score:** 0.2573600113391876"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attempt 3**\n",
    "\n",
    "- Removed activation hyperparameter. Set it to 'relu'\n",
    "- Changed hyperparameters for n_layers, first_layer_nodes, learning_rate\n",
    "\n",
    "**Best Hyperparameters--RandomSearch:**\n",
    "\n",
    "n_layers: 2\n",
    "\n",
    "first_layer_nodes: 350\n",
    "\n",
    "last_layer_nodes: 50\n",
    "\n",
    "learning_rate: 0.1\n",
    "\n",
    "**Best Score:** 0.3113600015640259"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attempt 4**\n",
    "\n",
    "- Only searching first_layer_nodes and learning_rate\n",
    "    - Using findings from non-stretch goal searches and stretch goal searches to pick hyperparameters for this attempt\n",
    "- Keeping layers at 2\n",
    "\n",
    "**Best Hyperparameters--RandomSearch:**\n",
    "\n",
    "first_layer_nodes: 350\n",
    "\n",
    "learning_rate: 0.001\n",
    "\n",
    "**Best Score:** 0.2972399890422821"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attempt 5**\n",
    "\n",
    "- Switched up hyperparameter values\n",
    "\n",
    "**Best Hyperparameters--RandomSearch:**\n",
    "\n",
    "first_layer_nodes: 300\n",
    "\n",
    "learning_rate: 0.1\n",
    "\n",
    "**Best Score:** 0.32708001136779785"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_trials=15\n",
    "num_initial_points=5\n",
    "beta=5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_tuner_k5_1 = BayesianOptimization(\n",
    "                    create_model_k,\n",
    "                    objective='val_accuracy',\n",
    "                    max_trials=max_trials,\n",
    "                    hyperparameters=hp_k, # pass in our hyperparameter dictionary\n",
    "                    num_initial_points=num_initial_points, \n",
    "                    beta=beta, \n",
    "                    seed=567,\n",
    "                    directory='./keras-tuner-trial-stretch-goal',\n",
    "                    project_name='bayesian_optimization_5_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15 Complete [00h 00m 25s]\n",
      "val_accuracy: 0.29264000058174133\n",
      "\n",
      "Best val_accuracy So Far: 0.35124000906944275\n",
      "Total elapsed time: 00h 05m 38s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "bayesian_tuner_k5_1.search(X_train, y_train,\n",
    "               epochs=5,\n",
    "               validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in ./keras-tuner-trial-stretch-goal/bayesian_optimization_5_1\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "first_layer_nodes: 450\n",
      "learning_rate: 0.001\n",
      "Score: 0.35124000906944275\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "first_layer_nodes: 450\n",
      "learning_rate: 0.001\n",
      "Score: 0.3463200032711029\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "first_layer_nodes: 550\n",
      "learning_rate: 0.1\n",
      "Score: 0.32328000664711\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "first_layer_nodes: 600\n",
      "learning_rate: 0.1\n",
      "Score: 0.31380000710487366\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "first_layer_nodes: 300\n",
      "learning_rate: 0.075\n",
      "Score: 0.3067600131034851\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "first_layer_nodes: 300\n",
      "learning_rate: 0.001\n",
      "Score: 0.3041599988937378\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "first_layer_nodes: 450\n",
      "learning_rate: 0.001\n",
      "Score: 0.2951200008392334\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "first_layer_nodes: 450\n",
      "learning_rate: 0.001\n",
      "Score: 0.29412001371383667\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "first_layer_nodes: 600\n",
      "learning_rate: 0.1\n",
      "Score: 0.29264000058174133\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "first_layer_nodes: 450\n",
      "learning_rate: 0.001\n",
      "Score: 0.28968000411987305\n"
     ]
    }
   ],
   "source": [
    "bayesian_tuner_k5_1.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Results\n",
    "  \n",
    " \n",
    "**Attempt 1**\n",
    "\n",
    "**Best Hyperparameters--Bayesian Optimization:**\n",
    "\n",
    "n_layers: 3\n",
    "\n",
    "first_layer_nodes: 300\n",
    "\n",
    "last_layer_nodes: 100\n",
    "\n",
    "learning_rate: 0.1\n",
    "\n",
    "activation: relu\n",
    "\n",
    "**Best Score:** 0.25172001123428345\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attempt 2**\n",
    "\n",
    "- Increased number of epochs and trials\n",
    "\n",
    "**Best Hyperparameters--Bayesian Optimization:**\n",
    "\n",
    "n_layers: 3\n",
    "\n",
    "first_layer_nodes: 300\n",
    "\n",
    "last_layer_nodes: 50\n",
    "\n",
    "learning_rate: 0.1\n",
    "\n",
    "activation: relu\n",
    "\n",
    "**Best Score:** 0.24028000235557556"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attempt 3**\n",
    "\n",
    "- Changed beta to 5\n",
    "- Removed activation hyperparameter. Set it to 'relu'\n",
    "- Changed hyperparameters for n_layers, first_layer_nodes, learning_rate\n",
    "\n",
    "**Best Hyperparameters--Bayesian Optimization:**\n",
    "\n",
    "n_layers: 2\n",
    "\n",
    "first_layer_nodes: 250\n",
    "\n",
    "last_layer_nodes: 50\n",
    "\n",
    "learning_rate: 0.05\n",
    "\n",
    "**Best Score:** 0.33191999793052673"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attempt 4**\n",
    "\n",
    "- Only searching first_layer_nodes and learning_rate\n",
    "    - Using findings from non-stretch goal searches and stretch goal searches to pick hyperparameters for this attempt\n",
    "- Keeping layers at 2\n",
    "\n",
    "**Best Hyperparameters--Bayesian Optimization:**\n",
    "\n",
    "first_layer_nodes: 275\n",
    "\n",
    "learning_rate: 0.075\n",
    "\n",
    "**Best Score:** 0.3412800133228302"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attempt 5**\n",
    "\n",
    "- Switched up hyperparameter values\n",
    "\n",
    "**Best Hyperparameters--Bayesian Optimization:**\n",
    "\n",
    "first_layer_nodes: 450\n",
    "\n",
    "learning_rate: 0.001\n",
    "\n",
    "**Best Score:** 0.35124000906944275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_433_Tune_Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "unit4sprint2",
   "language": "python",
   "name": "unit4sprint2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "nteract": {
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
