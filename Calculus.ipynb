{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Calculus \n",
    "\n",
    "In this notebook, you will be introduced to Ordinary Derivatives and multi-dimensional derivatives knowns Gradients. \n",
    "\n",
    "This notebook assumes that you've never taken a Calculus course before. However, even if you have, reviewing this material will be of great help to re-kindle your memory in preparation for the lesson on Gradient Descent and back-propagation in Sprint 2 Module 2. \n",
    "\n",
    "## Learning Objectives \n",
    "\n",
    "By the end of the notebook students will be able to explain the following in their own words\n",
    "\n",
    "- The concept of a Derivative \n",
    "- Differential notation $\\frac{dy}{dx}$ \n",
    "- The Gradient operator $\\nabla$ \n",
    "- The notation of partial derivatives ${\\displaystyle {\\frac {\\partial f}{\\partial x}}\\mathbf {i} +{\\frac {\\partial f}{\\partial y}}\\mathbf {j} }$\n",
    "- Using Tensorflow's GradientTape to take derivatives \n",
    "- The relationship between 3D surfaces and 2D contour maps \n",
    "- The Chain Rule in Calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.lib.display import YouTubeVideo\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What this video on The Essence of Calculus \n",
    "\n",
    "The take away from this video is to develop a conceptual understanding of Calculus before we dive into the notation in the next video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('WUvTyaaNkzM',  width=1000, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watch this video on the Derivative \n",
    "\n",
    "Now that we have watched the previous video which helped us build a conceptual understanding of Calculus. Let's what this next video to get familiar with the notation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('9vKqVkMQHKk',  width=1000, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now you've watched both videos and you kinda understand what a derivative is...kinda, right?\n",
    "\n",
    "The next section is going to show us how to actually calculate the derivative of a polynomial by hand and using Tensorflow. \n",
    "\n",
    "In our lesson on Gradient Descent and back-propagation, tensorflow will be doing all the calculus for us. \n",
    "\n",
    "However, calculus is at the heart of Gradient Descent. And Gradient Descent puts the \"learning\" in \"Machine Learning\". \n",
    "\n",
    "### Derivative of a Polynomial \n",
    "\n",
    "There are many types of functions that we could apply the derivative towards but let's focus on the simplest of them all. \n",
    "\n",
    "\n",
    "Here's the general formula for an Nth degree polynomial with N many terms. **\"Nth degree polynomial\"** means that there are N many $x's$ in the equation. And $N$ is a variable that we select.\n",
    "\n",
    "\n",
    "$$ y = f(x) = x^{n} +  x^{n-1}  + x^{n-2} +\\cdots +  x^{3} + x^{2}  +  x^{1} $$\n",
    "\n",
    "\n",
    "Here's the general formula for an Nth degree polynomial with just the leading Nth term.\n",
    "\n",
    "$$ y = f(x) = x^{n}$$\n",
    "\n",
    "Here's the general formula for the derivative of an Nth degree polynomial with just the leading Nth term.\n",
    "\n",
    "$${\\displaystyle \\frac{dy}{dx}  = {\\frac {d}{dx}}x^{n}=nx^{n-1}}$$\n",
    "\n",
    "Let's take the derivative of a few commonly occurring special cases  in order to get comfortable making the mental connection between the equations and the geometry. Remember that Tensorflow will be doing the calculus for us but we need to be able to look at a plot and draw certain conclusions from it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's define the simplist polynomial by setting $n~=~0$\n",
    "\n",
    "\n",
    "Any variable to the power of zero is 1.\n",
    "\n",
    "$$y = x^n = x^0 = 1$$\n",
    "\n",
    "Take the derivative. \n",
    "\n",
    "\n",
    "$$ \\frac{d}{dx} [y = 1 ]$$\n",
    "\n",
    "$$ \\frac{dy}{dx} = \\frac{d}{dx}1 $$\n",
    "\n",
    "\n",
    "$$ \\frac{dy}{dx} = 0$$\n",
    "\n",
    "**The derivative of a constant value is always zero.** In other words, the slope is a constant of zero, it doesn't change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_a_flat_line():\n",
    "    \"\"\"\n",
    "    Plots the function y = f(x) = 1\n",
    "    \"\"\"\n",
    "    y = np.linspace(1, 1, 21)\n",
    "    x = np.arange(-10, 11, 1)\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.title(\"This is what y = 1 looks like\")\n",
    "    plt.grid()\n",
    "    plt.plot(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below is the geometry of the equation $y = f(x) = 1$\n",
    "\n",
    "Remember that $ \\frac{dy}{dx}$ can be read as $ \\frac{dy}{dx} \\approx \\frac{\\text{rise}}{\\text{run}}$\n",
    "\n",
    "Ask yourself, what's the value of $\\text{rise}$?\n",
    "\n",
    "It's zero for a flat line. Hence the derivative is zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_a_flat_line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Let's set $n~=~1$\n",
    "\n",
    "$$y = x^n = x^1$$\n",
    "\n",
    "Take the derivative.\n",
    "\n",
    "$$ \\frac{d}{dx} [y = x ]$$\n",
    "\n",
    "$$ \\frac{dy}{dx} = \\frac{d}{dx}x = 1 \\cdot x^0 = 1 $$\n",
    "\n",
    "\n",
    "\n",
    "**The derivative of a variable to the power of 1 is always 1.** In order words, the derivative of y is a constant of 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_y_equals_x():\n",
    "    \"\"\"\n",
    "    Plots the function y = f(x) = x\n",
    "    \"\"\"\n",
    "    # the actual values in y and x are arbitrary\n",
    "    # I just thought to myself how I can plot something simple\n",
    "    y = np.arange(0, 6, 1)\n",
    "    x = np.arange(0, 6, 1)\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.title(\"This is what y = 1 looks like\")\n",
    "    plt.xticks(x)\n",
    "    plt.yticks(y)\n",
    "    plt.grid()\n",
    "    plt.plot(x, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_y_equals_x()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the plot of $y = f(x) = x$\n",
    "\n",
    "You can read the value of the derivative from the plot by mentally filling in the values for  $ \\frac{dy}{dx} \\approx \\frac{\\text{rise}}{\\text{run}}$\n",
    "\n",
    "Do it now, count the number of steps for rise and the number of steps for run. \n",
    "\n",
    "You should get $\\frac{\\text{rise}}{\\text{run}} = \\frac{1}{1} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Let's set $n~=~2$\n",
    "\n",
    "$$y = x^n = x^2$$\n",
    "\n",
    "Now let's apply the derivative operator to find the derivative of the function \n",
    "\n",
    "$$ \\frac{d}{dx} [y = x^2 ]$$\n",
    "\n",
    "$$ \\frac{dy}{dx} = \\frac{d}{dx}x^2 $$\n",
    "\n",
    "\n",
    "\n",
    "$$ \\frac{dy}{dx} = 2x^1 = 2x $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_parabola():\n",
    "    \"\"\"\n",
    "    Plots the parabola, y = f(x) = x^2\n",
    "    \"\"\"\n",
    "    # the actual values in y and x are arbitrary\n",
    "    # I just thought to myself how I can plot something simple\n",
    "    x = np.arange(-5, 5.25, .25)\n",
    "    y = x**2\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.title(\"This is what y = x^2 looks like\")\n",
    "    plt.grid()\n",
    "    plt.plot(x, y);\n",
    "    \n",
    "def plot_parabola_derivative():\n",
    "    \"\"\"\n",
    "    Plots the derivative of our parabola, dy/dx = 2x\n",
    "    \"\"\"\n",
    "    # the actual values in y and x are arbitrary\n",
    "    # I just thought to myself how I can plot something simple\n",
    "    x = np.arange(-5, 5.30, .25)\n",
    "    y = 2*x\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.title(\"This is what y = 2x looks like\")\n",
    "    plt.grid()\n",
    "    plt.plot(x, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parabola()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at a plot of a parabola $y = f(x) = x^2$ and trying to eyeball the derivative isn't as straight forward as the previous plot. So let's its derivative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parabola_derivative()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line $y = f(x) = 2x$ is the general function for determining the derivate for a parabola at any point, $x$. \n",
    "\n",
    "I'll admit that the connection between $y = f(x) = x^2$ and $\\frac{dy}{dx}  = 2x$ still doesn't seem as clear as the plot from the previous derivative we took. \n",
    "\n",
    "So let's plot the tangent line (i.e. the derivative) of the parabola directly on top of the parabola for a few points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parabola\n",
    "def f(x): \n",
    "    \"\"\"\n",
    "    Calculates a parabola y = f(x) = x^2\n",
    "    \"\"\"\n",
    "    return x**2\n",
    "\n",
    "# Define parabola derivative\n",
    "def calc_dy_dx(x): \n",
    "    \"\"\"\n",
    "    Calculates the tangent line (i.e. the derivative) of our parabola  dy/dx = 2x\n",
    "    \"\"\"\n",
    "    return 2*x\n",
    "\n",
    "def tangent_line(x, x1, y1):\n",
    "    \"\"\"\n",
    "    Formula for calculating the tagent line, y = m*(x - x1) + y1\n",
    "    \n",
    "    Notice that we replaced m with dy_dx because they are the same thing\n",
    "    \"\"\"\n",
    "    return calc_dy_dx(x1)*(x - x1) + y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define x data range for parabola\n",
    "x = np.linspace(-5,5,100)\n",
    "\n",
    "# points on the x-axis for which we want to calculate the slope of the parabola \n",
    "X = [-3, 0, 3]\n",
    "n_slopes = len(X)\n",
    "\n",
    "# Plot the figure\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.grid()\n",
    "plt.title(\"Parabola with {} tangent lines\".format(n_slopes))\n",
    "\n",
    "\n",
    "# store dy/dx values \n",
    "dy_dx_values = []\n",
    "for x1 in X:\n",
    "    # Choose point to plot tangent line\n",
    "    y1 = f(x1)\n",
    "\n",
    "    # Define x data range for tangent line\n",
    "    xrange = np.linspace(x1-1, x1+1, 10)\n",
    "    \n",
    "    # store derivate values \n",
    "    dy_dx_values.append(calc_dy_dx(x1))\n",
    "\n",
    "    plt.plot(x, f(x), \"b\", alpha=0.3)\n",
    "    plt.scatter(x1, y1, color='C1', s=50)\n",
    "    plt.plot(xrange, tangent_line(xrange, x1, y1), 'C1--', linewidth = 2)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the corresponding values of the derivatives of the parabola for 3 points along the x-axis\n",
    "dy_dx_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read this plot from left to right. \n",
    "\n",
    "The words slope, tangent line, derivative all mean the same thing, $$\\frac{dy}{dx}$$\n",
    "\n",
    "The first slope (i.e. derivative) is negative, it's -6. The 6 doesn't really matter what matters is the negative sign. It's telling us that the rate of change of the parabola is decreasing. Meaning that the parabola **is moving towards a minimum point in the curve.** \n",
    "\n",
    "The second slope is zero. This means that the parabola **is at a maximum or minimum point.**\n",
    "\n",
    "The third slope is positive, it's 6. The important part is the it's positive. This means that **it is moving away from the minimum point of the curve.** \n",
    "\n",
    "\n",
    "You might be wondering ... \n",
    "\n",
    "- Why are we concerned with theses tangent lines?\n",
    "\n",
    "- Why do we care about the slope at any given point of a parabola?\n",
    "\n",
    "- How is this connected to Machine Learning?\n",
    "\n",
    "\n",
    "This curve is the key behind Gradient Descent. But we won't make that connection explicit until the end of the notebook. \n",
    "\n",
    "For now, keep reading. Muahahahah!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Automatic Differentiation\n",
    "\n",
    "We took the derivative of a few key polynomial powers because they represent the 3 special causes\n",
    "- Derivative of a constant value\n",
    "- Derivative of a polynomial with a power of 1 \n",
    "- Derivative of a polynomial with a power greater than 1 \n",
    "\n",
    "In practice, we don't want to have to take these derivatives by hand. Especially when the functions become much more complicated. Luckly, Tensorflow will take derivatives for us. \n",
    "\n",
    "Let's use Tensorflow's `GradientTape()` to take a derivative of polynomials! \n",
    "\n",
    "Here's the [**Tensorflow documentation**](https://www.tensorflow.org/guide/autodiff) that explains how Tensorflow's `GradientTape()` works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow will treat `x` is a variable \n",
    "# but we still need to assign a value to it \n",
    "x = tf.constant(3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to convice us that Tensorflow is treating y as a function and not just as the result of squaring 3\n",
    "# let's calculate the derivative for y\n",
    "# we know thatthe derivative of y is 2x - we know because we calcualted it above \n",
    "\n",
    "with tf.GradientTape() as g:\n",
    "    # tensorflow requires that we specify which input variable to \"watch\"\n",
    "    g.watch(x)\n",
    "    \n",
    "    # now let's define y = x^2\n",
    "    y = x * x\n",
    "\n",
    "# now use the .gradient() operator to take the gradient of y = f(x)\n",
    "# we need to specify the independent variable (x) and the dependent variable (y)\n",
    "dy_dx = g.gradient(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the derivatiet of y = x^2 is dy/dx = 2x\n",
    "# 2 * (3) = 6\n",
    "# and 6 is the number that we get \n",
    "dy_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice that the actual value is located inside of an array\n",
    "dy_dx.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative of an Equation \n",
    "\n",
    "Let's take a derivative of simple equation -  a line. \n",
    "\n",
    "$$y = x \\cdot w + b$$\n",
    "\n",
    "What's the derivative of this line with respect to x?\n",
    "\n",
    "Note: `\"with respect to\"` will be written as `wrt`\n",
    "\n",
    "$$y = x \\cdot w + b$$\n",
    "\n",
    "$$ \\frac{d}{dx} [y = x \\cdot w + b]$$\n",
    "\n",
    "$$ \\frac{dy}{dx} = \\frac{d}{dx}[x \\cdot w + b] $$\n",
    "\n",
    "The derivative operator gets distributed to both terms seperated by the plus sign. \n",
    "But because we are taking the derivative wrt $x$ ( hence the notation of the operator having $x$ in the demoninator) we treat all other variables as constants. \n",
    "\n",
    "**The derivative of a constant value is always zero.**\n",
    "\n",
    "\n",
    "$$ \\frac{dy}{dx} = \\frac{d}{dx}x \\cdot w + \\frac{d}{dx}b $$\n",
    "\n",
    "Pull the constant (i.e. $w$) in front of the derivative operator to indicate that it's not part of the derivative.\n",
    "\n",
    "$$ \\frac{dy}{dx} = w \\cdot \\frac{d}{dx}x  + \\frac{d}{dx}b $$\n",
    "\n",
    "$$ \\frac{dy}{dx} = 1 \\cdot w + 0 $$\n",
    "\n",
    "$$ \\frac{dy}{dx} =  w  $$\n",
    "\n",
    " $w$ is the slope of this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable([5.], name=\"w\")\n",
    "b = tf.Variable([10.], name='b')\n",
    "x = tf.Variable([1.])\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = x * w + b\n",
    "    \n",
    "dy_dx = tape.gradient(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and we see that the derivative of this line is indeed w, which has a value of 5 \n",
    "dy_dx.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What if we want to calculate the derivative of this equation wrt w?**\n",
    "\n",
    "Can we even do that? \n",
    "\n",
    "Yes, we can!\n",
    "$$y = x \\cdot w + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = x \\cdot w + b$$\n",
    "\n",
    "$$ \\frac{d}{dw} [y = x \\cdot w + b]$$\n",
    "\n",
    "$$ \\frac{dy}{dw} = \\frac{d}{dw}[x \\cdot w + b] $$\n",
    "\n",
    "The derivative operator gets distributed to both terms seperated by the plus sign. \n",
    "But because we are taking the derivative wrt $w$ ( hence the notation of the operator having $w$ in the demoninator) we treat all other variables as constants. \n",
    "\n",
    "**The derivative of a constant value is always zero.**\n",
    "\n",
    "$$ \\frac{dy}{dw} = x \\cdot \\frac{d}{dw}w + \\frac{d}{dw}b $$\n",
    "\n",
    "$$ \\frac{dy}{dw} = x \\cdot 1 + 0 $$\n",
    "\n",
    "$$ \\frac{dy}{dw} =  x  $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable([5.], name=\"w\")\n",
    "b = tf.Variable([10.], name='b')\n",
    "x = tf.Variable([1.])\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = x * w + b\n",
    "    \n",
    "dy_dw = tape.gradient(y, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and we see that the derivative of this line is indeed x, which has a value of 1\n",
    "dy_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Vector Calculus\n",
    "\n",
    "As you can see we can take the derivative between any 2 variables that we care about. Not only that, we are not limited to just 2 variables either. Which brings us to Vector Calculus. \n",
    "\n",
    "In Vector Calculus, the derivative operator is given a different name - it's the Gradient. And the Graident has a different notation.\n",
    "\n",
    "Let's define $z$ as a function that has 2 dependent variables\n",
    "\n",
    "$$z = f(x, y) = \\frac{x^2}{a} + \\frac{y^2}{b} ~~~~a,b \\in \\mathbb{R} ~~\\text{a and b are real numbers}$$\n",
    "\n",
    "This is a very special function. It's called a **paraboloid**. The paraboloid is what a parabola looks like in 3 dimenisons.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_parabolid():\n",
    "    def f(x, y):\n",
    "        \"\"\"\n",
    "        Calculates a paraboloid z = f(x, y) = x^2/a + x^2/b\n",
    "        where a and b are both 1\n",
    "        \"\"\"\n",
    "        return x ** 2 + y ** 2\n",
    "\n",
    "    # create values for x, y, and z\n",
    "    x = np.arange(-10, 11, 1)\n",
    "    y =  np.arange(-10, 11, 1)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "\n",
    "    # plot the paraboloid\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = plt.axes(projection='3d')\n",
    "\n",
    "    ax.plot_surface(X, Y, Z, rstride=1, cstride=1,\n",
    "                    cmap='viridis', edgecolor='none')\n",
    "\n",
    "    ax.set_title('paraboloid');\n",
    "\n",
    "    ax.set_xlabel('X axis')\n",
    "    ax.set_ylabel('Y axis')\n",
    "    ax.set_zlabel('Z axis')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parabolid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the paraboloid to study Gradient Descent, so it's important to get comfortable with it. \n",
    "\n",
    "### Back to the Gradient\n",
    "\n",
    "The Gradient is an operator that operators on a function and the result is a vector where each component is a partial derivative. \n",
    "\n",
    "In the case of a function like $z = f(x, y)$ the gradient is \n",
    "\n",
    "$${\\displaystyle \\nabla f={\\frac {\\partial f}{\\partial x}}\\mathbf {i} +{\\frac {\\partial f}{\\partial y}}\\mathbf {j} }$$\n",
    "\n",
    "$~\\mathbf{i} ~~ \\text{and}~~ \\mathbf {j} ~$ are basis vectors that stand in for the $x$ and $y$ axis, respectively.\n",
    "\n",
    "The way to read the gradient conponents are as follow:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial x}$ represents how $z$ changes in the $x$ direction\n",
    "\n",
    "$\\frac{\\partial f}{\\partial y}$ represents how $z$ changes in the $y$ direction\n",
    "\n",
    "In case you're wondering, yes I am using $z$ and $f$ interchangably. Yes, you can do that.\n",
    "\n",
    "In the previous section, we only had $x$ and $y$ and so there really wasn't a need for the concept of a partial derivative because there is only **the** derivative when you are only playing with two variables. But when you have 3 or 4 or 5 or $N$ variables then introducing the concept of a partial derivatives because very useful and necessary. \n",
    "\n",
    "This next videos explores the concept of a partial derivative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watch this video on Partial Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('ly4S0oi3Yz8',  width=1000, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take the Gradient\n",
    "\n",
    "Now that we have introduced the core concepts for vector calculus, let' use Tensorflow to calculate the gradient of a paraboloid. \n",
    "\n",
    "Recall that the paraboloid in 3D space looks like this\n",
    "\n",
    "$$z = f(x, y) = \\frac{x^2}{a} + \\frac{y^2}{b} ~~~~a,b \\in \\mathbb{R} ~~\\text{a and b are real numbers}$$\n",
    "\n",
    "If we set $a=b=1$ for simplicity, then we have\n",
    "\n",
    "$$z = f(x, y) = x^2 + y^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Let's calculate the partial derivative wrt to x.** \n",
    "\n",
    "$$\\frac{\\partial{f}}{\\partial{x}} = \\frac{\\partial{f}}{\\partial{x}}[x^2 + y^2]$$\n",
    "\n",
    "$$\\frac{\\partial{f}}{\\partial{x}} = \\frac{\\partial{f}}{\\partial{x}}x^2 + \\frac{\\partial{f}}{\\partial{x}}y^2$$\n",
    "\n",
    "$$\\frac{\\partial{f}}{\\partial{x}} = 2x^1 + 0$$\n",
    "\n",
    "$$\\frac{\\partial{f}}{\\partial{x}} = 2x$$\n",
    "\n",
    "**Let's calculate the partial derivative wrt to y.**\n",
    "\n",
    "$$\\frac{\\partial{f}}{\\partial{y}} = \\frac{\\partial{f}}{\\partial{y}}[x^2 + y^2]$$\n",
    "\n",
    "$$\\frac{\\partial{f}}{\\partial{y}} = \\frac{\\partial{f}}{\\partial{y}}x^2 + \\frac{\\partial{f}}{\\partial{y}}y^2$$\n",
    "\n",
    "$$\\frac{\\partial{f}}{\\partial{y}} = 0 + 2y^1 $$\n",
    "\n",
    "$$\\frac{\\partial{f}}{\\partial{y}} = 2y$$\n",
    "\n",
    "\n",
    "If you were to take a knife and cut a slice down the middle that was parallel to the x-axis and a second slice parallel to the y-axis, then you would have identical looking slices. Hence, we have identical resutls for both partial derivatives. \n",
    " \n",
    "Again, we are using $z$ and $f$ interchangably because $z = f(x, y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable([[-1., 0., 1.]], name=\"x\")\n",
    "y = tf.Variable([[-1., 0., 1.]], name='y')\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = x**2 + y**2\n",
    "    \n",
    "dz_dx = tape.gradient(z, x)\n",
    "dz_dy = tape.gradient(z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dz/dx = 2x\n",
    "# thus 2x = 2 * [-1, 0, 1] = [-2, 0, 2]\n",
    "dz_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dz/dy = 2y\n",
    "# thus 2y = 2 * [-1, 0, 1] = [-2, 0, 2]\n",
    "dz_dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parabolid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Gradient Descent Preview\n",
    "\n",
    "\n",
    "\n",
    "### Our ML model is a Linear Regression \n",
    "\n",
    "Let's say we are training a simple Linear Regression model with one input feature, $x$.\n",
    "\n",
    "$$\\hat {y} = f(x) = x \\cdot w + b $$\n",
    "\n",
    "Where $\\hat {y}$ is our model's approximation (prediction) of the true $y$ value and $w$ is the slope. \n",
    "\n",
    "In fact $w$ can be expressed as $$ \\frac{dy}{dw} =  x  $$ which we saw earlier. \n",
    "\n",
    "We also saw earlier that a Gradient can be expressed as \n",
    "\n",
    "$${\\displaystyle \\nabla f={\\frac {\\partial f}{\\partial x}}\\mathbf {i} +{\\frac {\\partial f}{\\partial y}}\\mathbf {j} }$$\n",
    "\n",
    "$~\\mathbf{i} ~~ \\text{and}~~ \\mathbf {j} ~$ are basis vectors that stand in for the $x$ and $y$ axis, respectively.\n",
    "\n",
    "\n",
    "If we take the gradient of our Linear Regression wrt to $x$, then we get \n",
    "\n",
    " $$\\hat y = f(x)$$\n",
    "\n",
    "\n",
    "$${\\displaystyle \\nabla f={\\frac {\\partial f}{\\partial x}}\\mathbf {i} +{\\frac {\\partial f}{\\partial y}}\\mathbf {j} }$$\n",
    "\n",
    "\n",
    "$${\\displaystyle \\nabla f={\\frac {\\partial f}{\\partial x}}\\mathbf {i} + 0 \\mathbf {j} }$$\n",
    "\n",
    "$${\\displaystyle \\nabla f={\\frac {\\partial f}{\\partial x}} = \\frac{dy}{dx}}$$\n",
    "\n",
    "\n",
    "**Take Away:**\n",
    "We can express the partial derivate as an ordinary derivate because they are equivalent here; there are only 2 variables and so there is only a single component to the gradient. We can keep the gradient notation $\\nabla$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient of the Loss Function \n",
    "\n",
    "We can take the gradient of any equation we want. \n",
    "\n",
    "In fact, on our lesson on Gradient Descent we'll learn that we need to take the gradient of the loss function (which measures the error in our Linear Regression's predictions). \n",
    "\n",
    "We'll be using the Mean Square Error loss function (loss function and error function are the exact same thing, two names for the same thing). \n",
    "\n",
    "$${\\displaystyle \\operatorname {MSE} =f(x, y, w, b )={\\frac {1}{n}}\\sum _{i=1}^{n}(y_{i}-{\\hat {y_{i}}})^{2}.}$$\n",
    "\n",
    "The above $\\operatorname{MSE} =f(x, y, w, b )$ might make more sense if you remember that we can define $\\hat {y}$ as the following \n",
    "\n",
    "$$\\hat {y} = f(w, b) = x \\cdot w + b $$\n",
    "\n",
    "We can just define MSE as being a function of any variable its equation $x$, $y$, $w$, or $b$. \n",
    "\n",
    "For Gradient Descent purposes, we are interested in relating $f$ with the model parameters $w$ and $b$.\n",
    "\n",
    "\n",
    "$${\\displaystyle \\operatorname {MSE} =f(w)={\\frac {1}{n}}\\sum _{i=1}^{n}(Y_{i}-{{f(w, b)_{i}}})^{2}.}$$\n",
    "\n",
    "Here's the derivative of the loss function wrt $w$.\n",
    "\n",
    "$$  \\frac{\\partial f}{\\partial w} = \\frac{1}{n} \\sum_{i} -2( y_i - (w \\cdot x_i + b ))$$\n",
    "\n",
    "here $i$ is the $ith$ data point in the data set and $n$ is the number of data points in the data set.\n",
    "\n",
    "What this partial derivative is saying is that the gradient of the loss function is given by plugging in all the data points into the expression on the right hand side of the sigma $\\sum$ which sums up the results for all the data points then taking the average by dividing by $n$. \n",
    "\n",
    "Once we do that, we then subtract the gradient from the whatever value $w$ is currently storing. \n",
    "\n",
    "$$w_{new} = w_{current} - \\lambda \\cdot \\frac{\\partial f}{\\partial w} $$\n",
    "\n",
    "\n",
    "Sometimes the above equation has the partial derivative symbol, sometimes it has the gradient symbol \n",
    "\n",
    "\n",
    "$$w_{new} = w_{current} - \\lambda \\cdot \\nabla f $$\n",
    "\n",
    "$\\lambda$ is called the Learning Rate and we'll talk about in our lesson. \n",
    "\n",
    "**The above equation is Gradient Descent in algebraic form.**\n",
    "\n",
    "It subtracts the gradient (the change in error wrt to the change in $w$) from the current value in $w$ and replaces it with a new value. It keeps doing this until we arrive at a value for w that minimizes the error in our model predictions. \n",
    "\n",
    "**Here's what Gradient Descent looks like in it's Geometric form.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# points on the x-axis for which we want to calculate the slope of the parabola \n",
    "X = [4, 2, 0]\n",
    "n_slopes = len(X)\n",
    "\n",
    "# Define x data range for parabola\n",
    "x = np.linspace(-5,5,100)\n",
    "\n",
    "# Plot the figure\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.grid()\n",
    "plt.title(\"Loss Function vs. Model Weight\")\n",
    "\n",
    "plt.xlabel(\"feature weight (w)\")\n",
    "plt.ylabel(\"Loss Function (MSE)\")\n",
    "\n",
    "# store dy/dx values \n",
    "dy_dx_values = []\n",
    "color = [\"r\", \"g\", \"m\"]\n",
    "time = [1,2,3]\n",
    "for x1, c, t in zip(X,color, time):\n",
    "    \n",
    "    # Choose point to plot tangent line\n",
    "    y1 = f(x1)\n",
    "\n",
    "    # Define x data range for tangent line\n",
    "    xrange = np.linspace(x1-.75, x1+.75, 2)\n",
    "    \n",
    "    # store derivate values \n",
    "    dy_dx_values.append(calc_dy_dx(x1))\n",
    "\n",
    "    plt.plot(x, f(x), \"b\", alpha=0.3)\n",
    "    plt.scatter(x1, y1,color=c, s=50, label=\"t_{}\".format(t))\n",
    "    plt.plot(xrange, tangent_line(xrange, x1, y1), '{0}--'.format(c), linewidth = 2)\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function for our Linear Regression model is the MSE\n",
    "\n",
    "$${\\displaystyle \\operatorname {MSE} ={\\frac {1}{n}}\\sum _{i=1}^{n}(y_{i}-{\\hat {y_{i}}})^{2}.}$$\n",
    "\n",
    "\n",
    "Notice that the loss function squares all the terms, so that's how we know the loss function is a **parabola**. \n",
    "\n",
    "\n",
    "Now, let's imagine that the feature weight $w$ is being updated 3 times by gradient descent, so by this formula\n",
    "\n",
    "$$w_{new} = w_{current} - \\lambda \\cdot \\nabla f $$\n",
    "\n",
    "The first update occurs at time step $1$ (the red dot in the plot). \n",
    "\n",
    "We pass data ($x$, $y$) through Linear Regression $\\hat {y}$,  so now ($x$, $y$) have actual values. \n",
    "\n",
    "We take the Gradient of $\\operatorname {MSE}$ then we update the value of $w_{current}$ which places it at the 4 on the horizontal number line. \n",
    "\n",
    "Then we repeat the process and updated the value of $w$ to 2. \n",
    "\n",
    "Then we repeat the process and updated the value of $w$ to 0. \n",
    "\n",
    "The reason why the values of $w$ are converging towards 0 is because it is $w~=~0$ that corresponds to the lowest possible error in our model predictions. \n",
    "\n",
    "$$w~=~0~~ \\text{corresponds to} ~~\\operatorname {MSE}~=~0$$ \n",
    "\n",
    "**When this happens we say that the value of $w$ has been optimized.**\n",
    "\n",
    "The optimized value of a model parameter is sometimes given the special notation $\\mathbf{w^*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-----\n",
    "# Preparation for Tomorrow \n",
    "\n",
    "Before we conclude this notebook, I'm including several other (short) videos that you need to watch in preparation for our lesson on Gradient Descent. We will cover the Gradient on a 3D surface, 3D surfaces, representing a 3D surface as a 2D contour map, and finally the Chain Rule in Calculus. \n",
    "\n",
    "Watch the following videos in order to learn about the topics just mentioned. \n",
    "\n",
    "\n",
    "### The Gradient - a multi-dimensional derivative\n",
    "[**This video visually explains the multi-dimensional derivative called the Gradient visually**](https://www.youtube.com/watch?v=GkB4vW16QHI) - and that's the take away. This point of this video is to help you understand the Gradient visually using 3D surfaces. \n",
    "\n",
    "### Contour Maps \n",
    "It is very common to visualize 3D surfaces as 2D contour maps - and we'll be making a lot of use of contour maps to understand Gradient Descent. Watch this video in order to [**understand the relationship between 3D surfaces and 2D contour maps**](https://www.youtube.com/watch?v=acdX4YamDtU)\n",
    "\n",
    "### The Calculus of Backpropagation \n",
    "\n",
    "Unlike standard Sklearn ML models, neural networks use not just Gradient Descent but also something called Back-propagation in order to learn from the data. [**In order to understand how back-propagation works, you need to understand the Chain Rule in Calculus**](https://www.youtube.com/watch?v=9yCtWfI_Vjg). The take away here is to understand how a partial derivative can be decomposed into a product of multiple derivatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Conclusion \n",
    "\n",
    "Ok, so what are you suppose to actually take away from this notebook, right?\n",
    "\n",
    "You won't be asked to calculate any derivatives by hand nor will you be asked to regurgitate equations in any quiz or exam. But you will be asked to explain Gradient Descent and back-propagation in your own words at some point.\n",
    "\n",
    "The take aways are for you to be able to explain in your own words \n",
    "\n",
    "- The concept of a Derivative \n",
    "- Differential notation $\\frac{dy}{dx}$ \n",
    "- The Gradient operator $\\nabla$ \n",
    "- The notation of partial derivatives ${\\displaystyle {\\frac {\\partial f}{\\partial x}}\\mathbf {i} +{\\frac {\\partial f}{\\partial y}}\\mathbf {j} }$\n",
    "- Using Tensorflow's GradientTape to take derivatives \n",
    "- The relationship between 3D surfaces and 2D contour maps \n",
    "- The Chain Rule in Calculus \n",
    "\n",
    "these concepts will pop up a lot in the study of **Gradient Descent**, **back-propagation**, and **Neural Networks** more broadly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unit4sprint2",
   "language": "python",
   "name": "unit4sprint2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
